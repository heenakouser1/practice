{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e6103b-42da-42c0-8097-b000b5b24110",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0cec81-357b-42fa-9b65-b1eb95f5d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing values in a dataset refer to the absence of data in one or more fields or attributes. They can occur due to various reasons, such as incomplete data collection, data entry errors, or data corruption.\n",
    "It is essential to handle missing values in a dataset because they can negatively impact the quality and accuracy of data analysis and \n",
    "machine learning models. Missing values can cause bias in statistical analysis and prediction models, leading to incorrect conclusions and inaccurate predictions. Additionally,\n",
    "most machine learning algorithms cannot handle missing values, and they may either fail to produce results or produce suboptimal results.\n",
    "Some algorithms that are not affected by missing values include decision trees, random forests, and gradient boosting algorithms such as XGBoost and \n",
    "LightGBM. These algorithms are capable of handling missing values by using a variety of techniques such as surrogate splits, imputation,\n",
    "or treating missing values as a separate category. However, it is always recommended to handle missing values appropriately to avoid introducing errors and biases in data analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b042f1-4eb5-4e8c-b999-0f20ed114ceb",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "756bcc85-3583-42e1-a132-f107d14536e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d60b1f3-9fe8-4d6a-b07a-0572716def69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>E</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>child</td>\n",
       "      <td>False</td>\n",
       "      <td>G</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>D</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>B</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "1           1       1  female  38.0      1      0  71.2833        C  First   \n",
       "3           1       1  female  35.0      1      0  53.1000        S  First   \n",
       "6           0       1    male  54.0      0      0  51.8625        S  First   \n",
       "10          1       3  female   4.0      1      1  16.7000        S  Third   \n",
       "11          1       1  female  58.0      0      0  26.5500        S  First   \n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...    ...   \n",
       "871         1       1  female  47.0      1      1  52.5542        S  First   \n",
       "872         0       1    male  33.0      0      0   5.0000        S  First   \n",
       "879         1       1  female  56.0      0      1  83.1583        C  First   \n",
       "887         1       1  female  19.0      0      0  30.0000        S  First   \n",
       "889         1       1    male  26.0      0      0  30.0000        C  First   \n",
       "\n",
       "       who  adult_male deck  embark_town alive  alone  \n",
       "1    woman       False    C    Cherbourg   yes  False  \n",
       "3    woman       False    C  Southampton   yes  False  \n",
       "6      man        True    E  Southampton    no   True  \n",
       "10   child       False    G  Southampton   yes  False  \n",
       "11   woman       False    C  Southampton   yes   True  \n",
       "..     ...         ...  ...          ...   ...    ...  \n",
       "871  woman       False    D  Southampton   yes  False  \n",
       "872    man        True    B  Southampton    no   True  \n",
       "879  woman       False    C    Cherbourg   yes  False  \n",
       "887  woman       False    B  Southampton   yes   True  \n",
       "889    man        True    C    Cherbourg   yes   True  \n",
       "\n",
       "[182 rows x 15 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row deletion  it drops the rows where the missing values occurs\n",
    "\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2223d5e-5c5c-46bb-92fb-ac276617d308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>Second</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex  sibsp  parch     fare   class    who  \\\n",
       "0           0       3    male      1      0   7.2500   Third    man   \n",
       "1           1       1  female      1      0  71.2833   First  woman   \n",
       "2           1       3  female      0      0   7.9250   Third  woman   \n",
       "3           1       1  female      1      0  53.1000   First  woman   \n",
       "4           0       3    male      0      0   8.0500   Third    man   \n",
       "..        ...     ...     ...    ...    ...      ...     ...    ...   \n",
       "886         0       2    male      0      0  13.0000  Second    man   \n",
       "887         1       1  female      0      0  30.0000   First  woman   \n",
       "888         0       3  female      1      2  23.4500   Third  woman   \n",
       "889         1       1    male      0      0  30.0000   First    man   \n",
       "890         0       3    male      0      0   7.7500   Third    man   \n",
       "\n",
       "     adult_male alive  alone  \n",
       "0          True    no  False  \n",
       "1         False   yes  False  \n",
       "2         False   yes   True  \n",
       "3         False   yes  False  \n",
       "4          True    no   True  \n",
       "..          ...   ...    ...  \n",
       "886        True    no   True  \n",
       "887       False   yes   True  \n",
       "888       False    no  False  \n",
       "889        True   yes   True  \n",
       "890        True    no   True  \n",
       "\n",
       "[891 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column deletion - it drops the columns where the missing values occur\n",
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b69de06c-12c6-4671-92e0-2a101c49d891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imputation techniques\n",
    "import pandas as pd\n",
    "\n",
    "#1.mean imputation techniques\n",
    "df['age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8c1ac81-c3bb-4936-8eee-5082e90c8484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      22.000000\n",
       "1      38.000000\n",
       "2      26.000000\n",
       "3      35.000000\n",
       "4      35.000000\n",
       "         ...    \n",
       "886    27.000000\n",
       "887    19.000000\n",
       "888    29.699118\n",
       "889    26.000000\n",
       "890    32.000000\n",
       "Name: mean1, Length: 891, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean=df['age'].mean()\n",
    "df['mean1']= df['age'].fillna(mean)\n",
    "df['mean1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ff7f6a8-935c-4986-a4f0-7ff5db14e8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mean1'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6dad8bd7-50c9-43ed-8aea-bb5549fb65b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imputation techniques\n",
    "import pandas as pd\n",
    "\n",
    "#1.median imputation techniques\n",
    "df['age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34915d85-eeeb-47a7-8210-4b16eb7f1eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      22.0\n",
       "1      38.0\n",
       "2      26.0\n",
       "3      35.0\n",
       "4      35.0\n",
       "       ... \n",
       "886    27.0\n",
       "887    19.0\n",
       "888    28.0\n",
       "889    26.0\n",
       "890    32.0\n",
       "Name: median, Length: 891, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median=df['age'].median()\n",
    "df['median']= df['age'].fillna(median)\n",
    "df['median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffcfbdb9-f512-43b3-a784-da13f9fff59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['median'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a0a3e00-1b74-4f26-a27f-78669dd6a792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'C', 'Q', nan], dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imputation techniques\n",
    "import pandas as pd\n",
    "\n",
    "#1.mode imputation techniques\n",
    "pd.unique(df['embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "46aba18e-d0cd-417d-a526-d537ff77c783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embarked'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cdf4982-a65c-4fb9-a47e-b264488425ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode = df['embaisnull'].mode()[0]\n",
    "mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b0d42a6-1f96-41af-919e-7157a1b0d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mode1'] = df['embarked'].fillna(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e12885b-21e7-4734-899d-e9d446d38bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mode1'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9497c2b5-2936-4c2d-b03b-fbce468116ee",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa921dc-78f3-43d9-b117-6f39a6f1b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Imbalanced data refers to a situation where the classes or categories in a dataset are not equally represented. This means that one\n",
    "or more classes have significantly fewer samples than the others. Imbalanced data is common in many real-world applications such as fraud detection, medical diagnosis, and rare event prediction.\n",
    "The problem with imbalanced data is that most machine learning algorithms are designed to assume that the classes are balanced, and\n",
    "they tend to perform poorly when applied to imbalanced data. This is because the algorithms tend to be biased towards the majority class, \n",
    "which can lead to poor performance on the minority class. \n",
    "For example, if a dataset contains 95% samples of Class A and only 5% samples of \n",
    "Class B, a classifier trained on this dataset is likely to predict most new examples as Class A, regardless of their actual class.\n",
    "If imbalanced data is not handled, it can lead to several problems, including:\n",
    "Poor performance: The performance of a classifier trained on imbalanced data is likely to be poor, particularly on the minority class. This\n",
    "can lead to false negatives and false positives, which can have serious consequences in some applications.\n",
    "\n",
    "Biased models: Imbalanced data can lead to biased models that are not representative of the true distribution of the data. This can result in poor generalization to new examples and can make the model less reliable.\n",
    "\n",
    "Overfitting: In imbalanced datasets, the model can learn to overfit on the majority class, which can lead to poor performance on the minority class.\n",
    "\n",
    "To handle imbalanced data, several techniques can be used, including:\n",
    "Resampling: This involves either oversampling the minority class or undersampling the majority class to create a balanced dataset.\n",
    "\n",
    "Cost-sensitive learning: This involves assigning different misclassification costs to different classes to reflect the imbalance in the data.\n",
    "\n",
    "Algorithmic modifications: This involves modifying the machine learning algorithm to handle imbalanced data directly, such as changing the threshold of a decision rule or using specialized classifiers designed for imbalanced data.\n",
    "\n",
    "By handling imbalanced data, we can improve the performance and reliability of machine learning models and ensure that they are more representative of the true distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b502c65-378f-4e2c-8927-121c205fbd30",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7efe5-faef-45da-86af-34ccacd38533",
   "metadata": {},
   "outputs": [],
   "source": [
    "Upsampling and downsampling are two common techniques used to handle imbalanced data in machine learning.\n",
    "Downsampling involves reducing the number of samples in the majority class to match the number of samples in the minority class. \n",
    "This can be done randomly or using more sophisticated techniques, such as clustering or instance selection. \n",
    "Downsampling is useful when the majority class has a large number of samples that can be safely removed without losing important information.\n",
    "For example, consider a dataset with 1000 samples of Class A and 100 samples of Class B. If we downsample Class A to 100 samples, \n",
    "we can create a balanced dataset with 100 samples of each class.\n",
    "Upsampling, on the other hand, involves increasing the number of samples in the minority class to match the number of samples in the \n",
    "majority class. This can be done by replicating existing samples in the minority class, or by generating new synthetic samples using \n",
    "techniques such as SMOTE (Synthetic Minority Over-sampling Technique). Upsampling is useful when the minority class has a small number of samples that cannot be safely removed, and when we want to avoid losing important information.\n",
    "For example, consider a dataset with 1000 samples of Class A and 100 samples of Class B. If we upsample Class B to 1000 samples using \n",
    "SMOTE, we can create a balanced dataset with 1000 samples of each class.\n",
    "Whether to use upsampling or downsampling depends on the specific dataset and problem at hand. In general, upsampling is preferred when\n",
    "the minority class is important and has important features that need to be preserved, while downsampling is preferred when the majority \n",
    "class is too large to process efficiently or contains a significant amount of irrelevant data.\n",
    "In summary, upsampling and downsampling are two techniques used to handle imbalanced data in machine learning. Upsampling involves increasing\n",
    "the number of samples in the minority class, while downsampling involves reducing the number of samples in the majority class. The choice of which technique to use depends on the specific dataset and problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503be2b-1ac7-4793-addd-31676259abbc",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f37e8-3a5f-4a1a-98d2-a15eb9ff39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data augmentation is a technique used to increase the size and diversity of a dataset by creating new synthetic examples based on the existing data. \n",
    "This technique is commonly used in machine learning to improve model performance, particularly in situations where the available dataset is small or imbalanced.\n",
    "One popular data augmentation technique is SMOTE (Synthetic Minority Over-sampling Technique). SMOTE is specifically designed to handle\n",
    "imbalanced datasets where the minority class has very few samples. SMOTE generates synthetic examples of the minority class by interpolating between pairs of minority class examples.\n",
    "The basic idea of SMOTE is to randomly select a minority class example and its k nearest neighbors, where k is a user-defined parameter.\n",
    "SMOTE then creates new synthetic examples by interpolating between the minority example and each of its k nearest neighbors. Specifically,\n",
    "SMOTE selects a random point along the line segment connecting the minority example and its nearest neighbor and adds this point as a new example to the dataset.\n",
    "This process is repeated until the desired number of synthetic examples has been generated. The result is a larger and more diverse dataset\n",
    "that includes synthetic examples of the minority class.\n",
    "SMOTE can be very effective in improving the performance of machine learning models on imbalanced datasets. By creating synthetic examples of the\n",
    "minority class, SMOTE can help to address the problem of class imbalance and ensure that the model is better able to generalize to new examples.\n",
    "However, it is important to note that SMOTE can also introduce some noise and overfitting in the data, particularly if the value of k is set too\n",
    "high. Therefore, it is important to carefully select the parameters of SMOTE and to evaluate its effectiveness using appropriate validation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dab186-1a69-4e9a-a7c4-3983d0d6dfd4",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e4a2f3-ee50-440a-b829-c4b855a39c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "Outliers are data points that are significantly different from other data points in a dataset. \n",
    "These data points can be either very high or very low in value, and they can have a significant impact on statistical analysis and machine learning models.\n",
    "It is essential to handle outliers because they can cause a number of problems, including:\n",
    "Skewed data distribution: Outliers can distort the data distribution, making it difficult to accurately interpret the data and identify patterns.\n",
    "\n",
    "Misleading statistical measures: Outliers can significantly affect statistical measures such as mean and standard deviation, leading to inaccurate or misleading results.\n",
    "\n",
    "Biased machine learning models: Outliers can have a disproportionate influence on the model training process, leading to biased models that perform poorly on new data.\n",
    "\n",
    "Reduced model performance: Outliers can cause overfitting, leading to reduced model performance and accuracy.\n",
    "\n",
    "There are several techniques that can be used to handle outliers in a dataset. Some of these techniques include:\n",
    "Visual inspection: One of the simplest ways to identify outliers is by visually inspecting the data using box plots, scatter plots, and other visualization techniques.\n",
    "\n",
    "Statistical methods: Statistical methods such as Z-score, interquartile range (IQR),\n",
    "and Tukey's method can be used to identify outliers based on their distance from the mean or median of the data.\n",
    "\n",
    "Machine learning algorithms: Some machine learning algorithms, such as isolation forest and local outlier factor, are specifically designed to identify outliers in a dataset.\n",
    "\n",
    "Data transformation: Data transformation techniques such as normalization and logarithmic scaling can be used to reduce the impact of outliers on statistical analysis and machine learning models.\n",
    "\n",
    "Handling outliers is essential for accurate and reliable data analysis and machine learning. By identifying and removing or \n",
    "reducing the impact of outliers, we can improve the quality and accuracy of our results, leading to better insights and more effective decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e77532e-c727-4a02-bbde-8e389b4190cd",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf95cf-d284-4e50-973d-bdf027c6929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several techniques that can be used to handle missing data in customer data analysis:\n",
    "Deletion: One simple approach is to simply delete any rows or columns with missing data. However, this approach can lead to loss of important information and reduce the size of the dataset.\n",
    "\n",
    "Imputation: Imputation involves replacing missing data with estimated values based on the available data. This can be done using techniques such as mean imputation, median imputation, mode imputation, and iterative imputation.\n",
    "\n",
    "Regression: Regression analysis can be used to predict missing values based on the available data.\n",
    "This approach can be particularly effective if there is a strong correlation between the missing variable and other variables in the dataset.\n",
    "\n",
    "Multiple imputation: Multiple imputation involves creating multiple imputed datasets and combining them to produce a final estimate of the missing values.\n",
    "This approach can be particularly effective if there is a significant amount of missing data in the dataset.\n",
    "\n",
    "Machine learning: Machine learning algorithms can be used to predict missing values based on the available data. This approach can be particularly effective if the dataset contains complex relationships between variables.\n",
    "\n",
    "The choice of technique will depend on the nature of the missing data, the size of the dataset, and the specific requirements of the analysis. \n",
    "It is important to carefully evaluate the effectiveness of each technique and to consider the potential impact of missing data on the analysis results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034aec47-9e8a-4f79-90b4-2677825f849c",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2eab34-37e0-44a2-acee-52f34786bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "When dealing with missing data, there are several strategies to determine if the missing data is missing at random or if there is a pattern to the missing data. Here are some of the most commonly used methods:\n",
    "Analyze missingness patterns: You can start by examining the missingness patterns in the data. Plotting the distribution of missing values by variable or\n",
    "by record can help identify patterns of missingness. If the missingness patterns are random or similar across all variables, then it is likely that the missing data is missing at random. However,\n",
    "if there are patterns in the missingness, such as specific variables having higher rates of missing values or specific values within a variable being more likely to be missing, this suggests that the missing data may be non-random.\n",
    "\n",
    "Correlation analysis: You can examine the correlation between the missingness of a variable and other variables in the dataset. \n",
    "If the missingness of a variable is not correlated with any other variable, then it is likely missing at random. However, if the missingness of a variable is correlated with other variables, it suggests that the missing data may be non-random.\n",
    "\n",
    "Imputation and analysis: Impute the missing values using various techniques and compare the results. If the results are consistent\n",
    "across multiple imputation techniques, then it suggests that the missing data is missing at random. However, if the results vary significantly depending on the imputation technique used, it suggests that the missing data may be non-random.\n",
    "\n",
    "Expert knowledge: Sometimes expert knowledge can help determine if the missing data is missing at random or not. For example, \n",
    "if you are studying the impact of a new medication, and patients who experience side effects are more likely to drop out of the study, then the missing data is likely not missing at random.\n",
    "\n",
    "Statistical tests: You can use statistical tests such as the Little’s MCAR test or Missing Completely at Random (MCAR) test to\n",
    "determine if the missing data is missing at random or not. These tests can help determine if the pattern of missing data can be explained by chance or if there is a systematic reason for the missing data.\n",
    "\n",
    "Overall, it's important to remember that determining the pattern of missing data is often a combination of these methods, and it may require some judgment to make a final determination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5313b7c1-396e-4ec7-8e9d-736acbcd3cad",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb13a4fb-f211-4e1b-81ae-9d6c681ff265",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dealing with imbalanced datasets is a common problem in machine learning, especially in medical diagnosis projects. Here are some strategies\n",
    "you can use to evaluate the performance of your machine learning model on an imbalanced dataset:\n",
    "Confusion matrix: A confusion matrix is a table that summarizes the performance of a classification model. It shows the true positive, \n",
    "false positive, true negative, and false negative rates. In the case of an imbalanced dataset, accuracy may not be a good metric to evaluate the model's performance.\n",
    "Instead, you can look at other metrics such as precision, recall, F1-score, and the area under the receiver operating characteristic (ROC) curve. These metrics are not\n",
    "affected by the class imbalance and provide a better evaluation of the model's performance.\n",
    "\n",
    "Resampling techniques: Resampling techniques can be used to balance the dataset. You can either oversample the minority class or undersample the majority class. \n",
    "Oversampling involves adding copies of the minority class to the dataset, while undersampling involves removing examples from the majority class. However, both\n",
    "techniques have some drawbacks. Oversampling can lead to overfitting, while undersampling can lead to a loss of information. One common resampling technique is \n",
    "SMOTE (Synthetic Minority Over-sampling Technique), which generates synthetic examples of the minority class.\n",
    "\n",
    "Ensemble methods: Ensemble methods combine multiple models to improve their performance. One common ensemble method is the bagging method, which involves training \n",
    "multiple models on different subsets of the dataset and averaging their predictions. Another common ensemble method is the boosting method, which involves training\n",
    "multiple models sequentially, with each subsequent model focusing on the errors of the previous model. Ensemble methods can help improve the performance of the model on imbalanced datasets.\n",
    "\n",
    "Cost-sensitive learning: Cost-sensitive learning involves assigning different costs to different types of errors. In the case of an imbalanced dataset, misclassifying \n",
    "a minority class example as a majority class example may be more costly than the opposite. By assigning different costs to different types of errors, the model can be \n",
    "trained to minimize the overall cost of errors rather than just the number of errors.\n",
    "\n",
    "Domain knowledge: Finally, domain knowledge can be used to improve the model's performance on an imbalanced dataset. For example, if the dataset contains demographic\n",
    "information, you can use this information to stratify the dataset and ensure that both classes are represented equally in each stratum.\n",
    "\n",
    "Overall, it's important to remember that there is no single best strategy for dealing with imbalanced datasets, and the best approach may depend on the specific \n",
    "dataset and problem at hand. It's often a combination of these techniques that leads to the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb26578-6a4f-4795-8ee4-43234ca71aa9",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a6c93b-3f2a-42ec-adca-b438a8272cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several methods that can be employed to balance an unbalanced dataset and down-sample the majority class. Here are a few possible approaches:\n",
    "Random under-sampling: This involves randomly removing instances from the majority class until the dataset is balanced. One potential drawback of this \n",
    "approach is that it may result in the loss of important information, particularly if the majority class contains important or rare examples that should be preserved.\n",
    "\n",
    "Cluster-based under-sampling: This method involves clustering the majority class instances and then selecting representative instances from each cluster. \n",
    "This can help to preserve important information in the majority class, while also reducing the imbalance.\n",
    "\n",
    "Tomek Links: This method is an under-sampling technique that identifies pairs of instances from different classes that are close to each other, and removes \n",
    "the majority class instance from each pair. By doing this, the Tomek Links method creates a clearer separation between the two classes.\n",
    "\n",
    "Edited Nearest Neighbors (ENN): This method is also an under-sampling technique that removes noisy or mislabeled instances by checking the class of each instance's nearest neighbors. \n",
    "If an instance's nearest neighbors are mostly from a different class, then the instance is removed. \n",
    "ENN can be applied after other under-sampling or over-sampling techniques to further improve the balance of the dataset.\n",
    "\n",
    "Ensemble-based methods: These methods involve training multiple models on different subsets of the data, and then combining the results to produce a final prediction. \n",
    "This can be particularly useful in cases where the dataset is highly imbalanced and standard methods may not be effective.\n",
    "\n",
    "It is important to note that there is no one \"best\" method for balancing an unbalanced dataset, and the choice of method will depend on the specific characteristics of the \n",
    "dataset and the goals of the analysis. It is also important to evaluate the performance of the chosen method on a validation set to ensure that it does not introduce biases or negatively impact the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed0e8e-a6ee-4d68-a085-25cc0068840c",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724e2ff0-f38b-42f7-a186-311bcc40b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "If I have an unbalanced dataset with a low percentage of occurrences of a rare event, you can employ various techniques to balance the \n",
    "dataset and up-sample the minority class. Here are a few possible approaches:\n",
    "Random over-sampling: This involves randomly duplicating instances from the minority class until the dataset is balanced. One potential drawback \n",
    "of this approach is that it may result in overfitting and lower the overall accuracy of the model.\n",
    "\n",
    "Synthetic minority over-sampling technique (SMOTE): This method involves creating synthetic instances of the minority class by interpolating \n",
    "between existing instances. SMOTE generates new instances \n",
    "by taking the difference between the feature vector of one minority class instance and its k-nearest neighbors, and then multiplying this difference\n",
    "by a random number between 0 and 1. This can help to balance the dataset while also preserving the overall distribution of the minority class.\n",
    "\n",
    "Adaptive Synthetic Sampling (ADASYN): This method is an extension of SMOTE that generates more synthetic instances in the minority class regions that\n",
    "are harder to learn by the classifier. The idea is to generate more synthetic samples where the density of the minority class is lower, thus focusing more on the difficult to learn samples.\n",
    "\n",
    "SMOTE-Tomek: This method combines the SMOTE over-sampling technique with Tomek Links under-sampling. Tomek Links are pairs of instances from different \n",
    "classes that are close to each other and can be removed to increase the separation between the classes. SMOTE-Tomek first applies applies SMOTE over-sampling \n",
    "to the remaining minority class instances., and then Tomek Links under-sampling to remove the majority class instances that form Tomek Links with minority class instances.\n",
    "\n",
    "SMOTE-ENN: This method combines the SMOTE over-sampling technique with Edited Nearest Neighbors (ENN) under-sampling. ENN is a cleaning technique that removes \n",
    "noisy or mislabeled instances by checking the class of each instance's nearest neighbors. SMOTE-ENN first applies SMOTE over-sampling to the minority class instances,\n",
    "and then applies ENN under-sampling to remove instances that are misclassified by their nearest neighbors.\n",
    "\n",
    "It is important to note that up-sampling the minority class can also lead to overfitting and reduced generalization performance. Therefore, it is important to evaluate\n",
    "the performance of the chosen method on a validation set to ensure that it does not introduce biases or negatively impact the accuracy of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
